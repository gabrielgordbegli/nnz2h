{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGHatCI51JP"
      },
      "source": [
        "# micrograd exercises\n",
        "\n",
        "1. watch the [micrograd video](https://www.youtube.com/watch?v=VMj-3S1tku0) on YouTube\n",
        "2. come back and complete these exercises to level up :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFt6NKOz6iBZ"
      },
      "source": [
        "## section 1: derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jx9fCXl5xHd",
        "outputId": "666086ad-0509-4bbf-a7d5-2f76d7f52654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.336362190988558\n"
          ]
        }
      ],
      "source": [
        "# here is a mathematical expression that takes 3 inputs and produces one output\n",
        "from math import sin, cos\n",
        "\n",
        "def f(a, b, c):\n",
        "  return -a**3 + sin(3*b) - 1.0/c + b**2.5 - a**0.5\n",
        "\n",
        "print(f(2, 3, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qXaH59eL9zxf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553390593273\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027111255\n",
            "OK for dim 2: expected 0.0625, yours returns 0.0625\n"
          ]
        }
      ],
      "source": [
        "# write the function df that returns the analytical gradient of f\n",
        "# i.e. use your skills from calculus to take the derivative, then implement the formula\n",
        "# if you do not calculus then feel free to ask wolframalpha, e.g.:\n",
        "# https://www.wolframalpha.com/input?i=d%2Fda%28sin%283*a%29%29%29\n",
        "\n",
        "def gradf(a, b, c):\n",
        "  return [-1 / (2 * (a ** 0.5)) - 3 * a**2, 2.5 * b**1.5 + 3 * cos(3 * b), 1 / (c ** 2)] # todo, return [df/da, df/db, df/dc]\n",
        "\n",
        "def df(a, b, c):\n",
        "  return \n",
        "  \n",
        "\n",
        "\n",
        "# expected answer is the list of \n",
        "ans = [-12.353553390593273, 10.25699027111255, 0.0625]\n",
        "yours = gradf(2, 3, 4)\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(yours[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_27n-KTA9Qla"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353559348809995\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.256991666679482\n",
            "OK for dim 2: expected 0.0625, yours returns 0.062499984743169534\n"
          ]
        }
      ],
      "source": [
        "# now estimate the gradient numerically without any calculus, using\n",
        "# the approximation we used in the video.\n",
        "# you should not call the function df from the last cell\n",
        "\n",
        "# -----------\n",
        "a = 2\n",
        "b = 3\n",
        "c = 4\n",
        "h = 0.000001\n",
        "fa = ((f(a + h, b, c) - f(a, b, c)) / h)\n",
        "fb = ((f(a, b + h, c) - f(a, b, c)) / h)\n",
        "fc = ((f(a, b, c + h) - f(a, b, c)) / h)\n",
        "\n",
        "numerical_grad = [fa, fb, fc] # TODO\n",
        "# -----------\n",
        "\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BUqsGb5o_h2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553390820336\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.256990271617639\n",
            "OK for dim 2: expected 0.0625, yours returns 0.06250000001983835\n"
          ]
        }
      ],
      "source": [
        "# there is an alternative formula that provides a much better numerical \n",
        "# approximation to the derivative of a function.\n",
        "# learn about it here: https://en.wikipedia.org/wiki/Symmetric_derivative\n",
        "# implement it. confirm that for the same step size h this version gives a\n",
        "# better approximation.\n",
        "\n",
        "# -----------\n",
        "a = 2\n",
        "b = 3\n",
        "c = 4\n",
        "h = 0.00001 #can use a 10x bigger step size and it will pass the test\n",
        "\n",
        "fa = ((f(a + h, b, c) - f(a - h, b, c)) / (2 * h))\n",
        "fb = ((f(a, b + h, c) - f(a, b - h, c)) / (2 * h))\n",
        "fc = ((f(a, b, c + h) - f(a, b, c - h)) / (2 * h))\n",
        "\n",
        "numerical_grad2 = [fa, fb, fc] # TODO\n",
        "# -----------\n",
        "\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad2[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tklF9s_4AtlI"
      },
      "source": [
        "## section 2: support for softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "nAPe_RVrCTeO"
      },
      "outputs": [],
      "source": [
        "# Value class starter code, with many functions taken out\n",
        "from math import exp, log\n",
        "\n",
        "\n",
        "class Value:\n",
        "\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data = data\n",
        "        self.grad = 0.0\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data})\"\n",
        "\n",
        "    def __add__(self, other):  # exactly as in the video\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += 1.0 * out.grad\n",
        "            other.grad += 1.0 * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ------\n",
        "    # re-implement all the other functions needed for the exercises below\n",
        "    # your code here\n",
        "    # TODO\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        if other == 0:\n",
        "            return self\n",
        "        else:\n",
        "            return self.__add__(other)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data / other.data, (self, other), '/')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += 1.0 / other.data * out.grad\n",
        "            other.grad += -self.data / (other.data ** 2) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data - other.data, (self, other), '-')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += 1.0 * out.grad\n",
        "            other.grad += -1.0 * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __neg__(self):\n",
        "        out = Value(-self.data, (self,), 'neg')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += -1.0 * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def exp(self):\n",
        "        out = Value(exp(self.data), (self,), 'exp')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += exp(self.data) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def log(self):\n",
        "        out = Value(log(self.data), (self,), 'log')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += 1.0 / self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ------\n",
        "\n",
        "    def backward(self):  # exactly as in video\n",
        "        topo = []\n",
        "        visited = set()\n",
        "\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        self.grad = 1.0\n",
        "        for node in reversed(topo):\n",
        "            node._backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "VgWvwVQNAvnI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Value(data=0.04177257051535045), Value(data=0.839024507462532), Value(data=0.005653302662216329), Value(data=0.11354961935990121)]\n",
            "2.1755153626167147\n",
            "OK for dim 0: expected 0.041772570515350445, yours returns 0.041772570515350445\n",
            "OK for dim 1: expected 0.8390245074625319, yours returns 0.8390245074625319\n",
            "OK for dim 2: expected 0.005653302662216329, yours returns 0.005653302662216329\n",
            "OK for dim 3: expected -0.8864503806400986, yours returns -0.886450380640099\n"
          ]
        }
      ],
      "source": [
        "# without referencing our code/video __too__ much, make this cell work\n",
        "# you'll have to implement (in some cases re-implemented) a number of functions\n",
        "# of the Value object, similar to what we've seen in the video.\n",
        "# instead of the squared error loss this implements the negative log likelihood\n",
        "# loss, which is very often used in classification.\n",
        "\n",
        "# this is the softmax function\n",
        "# https://en.wikipedia.org/wiki/Softmax_function\n",
        "def softmax(logits):\n",
        "  counts = [logit.exp() for logit in logits]\n",
        "  denominator = sum(counts)\n",
        "  out = [c / denominator for c in counts]\n",
        "  return out\n",
        "\n",
        "# this is the negative log likelihood loss function, pervasive in classification\n",
        "logits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\n",
        "probs = softmax(logits)\n",
        "print(probs)\n",
        "loss = -probs[3].log() # dim 3 acts as the label for this input example\n",
        "loss.backward()\n",
        "print(loss.data)\n",
        "\n",
        "ans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n",
        "for dim in range(4):\n",
        "  ok = 'OK' if abs(logits[dim].grad - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "q7ca1SVAGG1S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.0418,  0.8390,  0.0057, -0.8865])\n"
          ]
        }
      ],
      "source": [
        "# First, we need the torch library. It's a tool that helps us do math (like addition, multiplication, etc.) on big lists of numbers (which we call \"tensors\") all at once.\n",
        "import torch\n",
        "\n",
        "# We start with some numbers (we call these \"logits\"). We tell torch that we want to remember what we do with these numbers because we'll need that information later.\n",
        "logits_torch = torch.tensor([0.0, 3.0, -2.0, 1.0], requires_grad=True)\n",
        "\n",
        "# We then use a special function called \"softmax\" to turn our logits into probabilities. This makes the logits into numbers between 0 and 1 that add up to 1, like rolling a dice.\n",
        "probs_torch = torch.softmax(logits_torch, dim=0)\n",
        "\n",
        "# We then look at the probability of the fourth event (counting starts from zero, so it's at position 3). We take the negative of its log as our \"loss\". Think of it as a measure of how surprised we are if the fourth event happens.\n",
        "loss_torch = -probs_torch[3].log()\n",
        "\n",
        "# Now we ask torch to figure out how our loss changes when we nudge the logits a little bit. This is called \"computing the gradient\", and it's like figuring out which way is downhill if you are hiking.\n",
        "loss_torch.backward()\n",
        "\n",
        "# Finally, we print out these gradients we computed. These numbers tell us how to change our logits next time to make our loss smaller (like a hint to reach the bottom of the hill faster).\n",
        "print(logits_torch.grad)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
